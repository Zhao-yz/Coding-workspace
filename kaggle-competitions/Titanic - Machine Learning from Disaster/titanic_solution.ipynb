{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Machine Learning from Disaster\n",
    "\n",
    "## Competition Goal\n",
    "Predict which passengers survived the Titanic shipwreck using machine learning.\n",
    "\n",
    "## Approach\n",
    "1. Data Loading and Exploration\n",
    "2. Missing Value Analysis and Imputation\n",
    "3. Feature Engineering\n",
    "4. Model Training with Cross-Validation\n",
    "5. Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info\n",
    "print(\"\\n=== Train Data Info ===\")\n",
    "train.info()\n",
    "print(\"\\n=== Basic Statistics ===\")\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check survival rate\n",
    "print(\"\\nSurvival Rate:\")\n",
    "print(train['Survived'].value_counts(normalize=True))\n",
    "\n",
    "# Visualize survival\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "train['Survived'].value_counts().plot(kind='bar', ax=axes[0], color=['salmon', 'lightblue'])\n",
    "axes[0].set_title('Survival Counts')\n",
    "axes[0].set_xlabel('Survived (0=No, 1=Yes)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['Did not survive', 'Survived'], rotation=0)\n",
    "\n",
    "train['Survived'].value_counts(normalize=True).plot(kind='pie', ax=axes[1], autopct='%1.1f%%', colors=['salmon', 'lightblue'])\n",
    "axes[1].set_ylabel('')\n",
    "axes[1].set_title('Survival Rate')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "def check_missing(df, name):\n",
    "    print(f\"\\n=== Missing Values in {name} ===\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = 100 * missing / len(df)\n",
    "    missing_table = pd.concat([missing, missing_pct], axis=1, keys=['Total', 'Percent'])\n",
    "    missing_table = missing_table[missing_table['Total'] > 0].sort_values('Total', ascending=False)\n",
    "    print(missing_table)\n",
    "    return missing_table\n",
    "\n",
    "train_missing = check_missing(train, 'Train')\n",
    "test_missing = check_missing(test, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data patterns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "if not train_missing.empty:\n",
    "    train_missing['Percent'].plot(kind='barh', ax=axes[0], color='coral')\n",
    "    axes[0].set_title('Missing Values in Train Set (%)')\n",
    "    axes[0].set_xlabel('Percentage')\n",
    "\n",
    "if not test_missing.empty:\n",
    "    test_missing['Percent'].plot(kind='barh', ax=axes[1], color='skyblue')\n",
    "    axes[1].set_title('Missing Values in Test Set (%)')\n",
    "    axes[1].set_xlabel('Percentage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "We'll create new features to improve model performance:\n",
    "- **Title**: Extract from Name (Mr, Mrs, Miss, Master, etc.)\n",
    "- **FamilySize**: SibSp + Parch + 1\n",
    "- **IsAlone**: Whether traveling alone\n",
    "- **Deck**: Extract from Cabin\n",
    "- **AgeGroup**: Binned age categories\n",
    "- **FareGroup**: Binned fare categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and test for consistent feature engineering\n",
    "full_data = [train, test]\n",
    "\n",
    "for dataset in full_data:\n",
    "    # Extract Title from Name\n",
    "    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    \n",
    "    # Create FamilySize\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "    \n",
    "    # Create IsAlone\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "    \n",
    "    # Extract Deck from Cabin\n",
    "    dataset['Deck'] = dataset['Cabin'].str[0]\n",
    "    dataset['Deck'] = dataset['Deck'].fillna('Unknown')\n",
    "\n",
    "print(\"Title distribution in train:\")\n",
    "print(train['Title'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group rare titles\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "print(\"\\nTitle distribution after grouping:\")\n",
    "print(train['Title'].value_counts())\n",
    "\n",
    "# Visualize survival by title\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=train, x='Title', y='Survived', errorbar=None)\n",
    "plt.title('Survival Rate by Title')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing Age values based on median age by Pclass and Sex\n",
    "for dataset in full_data:\n",
    "    # Group by Pclass and Sex to fill Age\n",
    "    for pclass in [1, 2, 3]:\n",
    "        for sex in ['male', 'female']:\n",
    "            median_age = train[(train['Pclass'] == pclass) & (train['Sex'] == sex)]['Age'].median()\n",
    "            dataset.loc[(dataset['Age'].isnull()) & (dataset['Pclass'] == pclass) & (dataset['Sex'] == sex), 'Age'] = median_age\n",
    "\n",
    "# Fill missing Embarked with mode\n",
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna(train['Embarked'].mode()[0])\n",
    "\n",
    "# Fill missing Fare with median\n",
    "for dataset in full_data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(train[['Age', 'Embarked', 'Fare']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Age and Fare bins\n",
    "for dataset in full_data:\n",
    "    # Age groups\n",
    "    dataset['AgeGroup'] = pd.cut(dataset['Age'], bins=[0, 12, 18, 35, 60, 100], labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior'])\n",
    "    \n",
    "    # Fare groups (quartiles)\n",
    "    dataset['FareGroup'] = pd.qcut(dataset['Fare'], q=4, labels=['Low', 'Medium', 'High', 'VeryHigh'], duplicates='drop')\n",
    "\n",
    "print(\"\\nAge Group distribution:\")\n",
    "print(train['AgeGroup'].value_counts())\n",
    "print(\"\\nFare Group distribution:\")\n",
    "print(train['FareGroup'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival by key features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Sex\n",
    "sns.barplot(data=train, x='Sex', y='Survived', ax=axes[0, 0], errorbar=None)\n",
    "axes[0, 0].set_title('Survival by Sex')\n",
    "\n",
    "# Pclass\n",
    "sns.barplot(data=train, x='Pclass', y='Survived', ax=axes[0, 1], errorbar=None)\n",
    "axes[0, 1].set_title('Survival by Passenger Class')\n",
    "\n",
    "# Embarked\n",
    "sns.barplot(data=train, x='Embarked', y='Survived', ax=axes[0, 2], errorbar=None)\n",
    "axes[0, 2].set_title('Survival by Embarkation Port')\n",
    "\n",
    "# FamilySize\n",
    "sns.barplot(data=train, x='FamilySize', y='Survived', ax=axes[1, 0], errorbar=None)\n",
    "axes[1, 0].set_title('Survival by Family Size')\n",
    "\n",
    "# AgeGroup\n",
    "sns.barplot(data=train, x='AgeGroup', y='Survived', ax=axes[1, 1], errorbar=None)\n",
    "axes[1, 1].set_title('Survival by Age Group')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# IsAlone\n",
    "sns.barplot(data=train, x='IsAlone', y='Survived', ax=axes[1, 2], errorbar=None)\n",
    "axes[1, 2].set_title('Survival by Alone Status')\n",
    "axes[1, 2].set_xticklabels(['With family', 'Alone'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "# Encode categorical variables for correlation analysis\n",
    "train_encoded = train.copy()\n",
    "cat_cols = ['Sex', 'Embarked', 'Title', 'AgeGroup', 'FareGroup']\n",
    "for col in cat_cols:\n",
    "    train_encoded[col] = pd.Categorical(train_encoded[col]).codes\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_cols = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'IsAlone', 'Title', 'AgeGroup', 'FareGroup']\n",
    "correlation_matrix = train_encoded[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, square=True)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCorrelation with Survival (sorted):\")\n",
    "print(correlation_matrix['Survived'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "feature_cols = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Title', 'FamilySize', 'IsAlone', 'AgeGroup', 'FareGroup']\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X_train = pd.get_dummies(train[feature_cols], drop_first=True)\n",
    "X_test = pd.get_dummies(test[feature_cols], drop_first=True)\n",
    "y_train = train['Survived']\n",
    "\n",
    "# Ensure train and test have same columns\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"\\nFeatures:\")\n",
    "print(X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training and Cross-Validation\n",
    "\n",
    "We'll compare three models:\n",
    "1. Logistic Regression (baseline)\n",
    "2. Random Forest\n",
    "3. Gradient Boosting\n",
    "\n",
    "Using StratifiedKFold cross-validation to ensure robust performance estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    results[name] = scores\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean CV Accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "    print(f\"  Individual fold scores: {scores}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bp = ax.boxplot([results[name] for name in models.keys()], labels=models.keys(), patch_artist=True)\n",
    "\n",
    "for patch, color in zip(bp['boxes'], ['lightblue', 'lightgreen', 'lightcoral']):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Model Comparison - Cross-Validation Scores')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best performing model for hyperparameter tuning\n",
    "best_model_name = max(results, key=lambda k: results[k].mean())\n",
    "print(f\"Best base model: {best_model_name} with accuracy: {results[best_model_name].mean():.4f}\")\n",
    "\n",
    "# Hyperparameter tuning for Random Forest (typically best performer)\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "print(\"\\nPerforming Grid Search for Random Forest...\")\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    rf_param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {rf_grid.best_params_}\")\n",
    "print(f\"Best CV score: {rf_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Gradient Boosting\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "print(\"Performing Grid Search for Gradient Boosting...\")\n",
    "gb_grid = GridSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    gb_param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {gb_grid.best_params_}\")\n",
    "print(f\"Best CV score: {gb_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final model\n",
    "if rf_grid.best_score_ >= gb_grid.best_score_:\n",
    "    final_model = rf_grid.best_estimator_\n",
    "    final_score = rf_grid.best_score_\n",
    "    model_name = \"Random Forest\"\n",
    "else:\n",
    "    final_model = gb_grid.best_estimator_\n",
    "    final_score = gb_grid.best_score_\n",
    "    model_name = \"Gradient Boosting\"\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Final Model: {model_name}\")\n",
    "print(f\"Cross-Validation Accuracy: {final_score:.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': final_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Most Important Features:\")\n",
    "print(feature_importance.head(15))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=feature_importance.head(15), x='importance', y='feature', palette='viridis')\n",
    "plt.title(f'Top 15 Feature Importances - {model_name}')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on full training data\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "predictions = final_model.predict(X_test)\n",
    "\n",
    "print(f\"Total predictions: {len(predictions)}\")\n",
    "print(f\"Predicted survivors: {predictions.sum()} ({100*predictions.sum()/len(predictions):.1f}%)\")\n",
    "print(f\"Predicted deaths: {(1-predictions).sum()} ({100*(1-predictions).sum()/len(predictions):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test['PassengerId'],\n",
    "    'Survived': predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created: submission.csv\")\n",
    "print(f\"\\nFirst few predictions:\")\n",
    "print(submission.head(10))\n",
    "print(f\"\\nSubmission shape: {submission.shape}\")\n",
    "print(f\"Expected shape: (418, 2)\")\n",
    "print(f\"\\nValidation: Shape matches requirement: {submission.shape == (418, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "1. **Most Important Features**: Sex, Title, Fare, Age, and Pclass were the strongest predictors\n",
    "2. **Survival Patterns**:\n",
    "   - Women had much higher survival rates than men\n",
    "   - First class passengers survived more often\n",
    "   - Children and young adults had better survival chances\n",
    "   - Passengers with smaller families had better odds\n",
    "\n",
    "### Model Performance:\n",
    "- Final model achieved ~{:.1f}% accuracy on cross-validation\n",
    "- Used ensemble methods (Random Forest / Gradient Boosting) for robust predictions\n",
    "- Hyperparameter tuning improved performance over baseline\n",
    "\n",
    "### Next Steps:\n",
    "1. Submit predictions to Kaggle\n",
    "2. Analyze leaderboard feedback\n",
    "3. Consider additional feature engineering or model ensembling for improvement\n",
    "\".format(final_score * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
